---
layout: post
title:  k8s集群运维记录
description: 
modified:   2019-10-18 12:57:47
tags: [k8s]
---

## k8s集群排障命令
### node相关
1. 查看node事件
`kubectl describe node <node-name>`

### pod相关
1. 查看pod事件
`kubectl describe pod <pod-name>`

### 查看各组件日志
若k8s各组件是用systemd管理的，需要使用journalctl去查看各组件日志, 如果不是systemd管理，则去查看指定的日志文件
`journalctl -f -u kube-apiserver` journalctl的具体参数可以man
若k8s个组件是以pod运行的，参考[此文章][k8s]
```

```
[k8s]:https://zhuanlan.zhihu.com/p/34323536

## k8s集群高可用部署


### docker
1. node节点上docker pull image 时没反应
  检查镜像registry的地址是否能ping通, 如果ping不通，在/etc/hosts中加上地址的解析, 问题解决.

2. node节点上docker pull image, 报错`dial tcp 10.226.136.163:443: getsockopt: connection refused`
  经排查发现，dockerd 启动参数--config-file中的配置项错误，`insecure-registries:["docker02:35000"]`,
  如果docker需要从SSL镜像源管理镜像需要配置--insecure-registries参数，使得docker能从指定的地址中获取镜像。
  所以此时的配置是错误的，导致docker无法从指定的地址中获取镜像. 将此参数配置为镜像仓库地址，或者`0.0.0.0/0`表示允许从
  所有主机地址上. 问题解决.


### kubelet
1. 重装Node节点时，没有清理kubelet-client-current.pem，导致此软连接一致链接的旧的证书文件，导致kubelet上报api-server出现证书
问题

卸载节点时，要清理cert目录下的kubelet-client-current.pem

2. 集群新加node节点后，使用kubectl get nodes --watch命令查看，部分节点的status一直在由Ready->NotReady→Ready。
排查原因：
由于node节点上的kubelet上报状态时间，超过了controller-manager默认设置的监控节点状态的时间周期，导致在Ready的周期内没有收到状态上报，controller-manager将此节点的状态置为了NotReady，当controller-manager收到了迟到的状态上报后，又将节点状态置为了Ready

解决方案：
修改controller-manager的启动参数--node-monitor-grace-period，默认是40s，改为了300s。--node-monitor-period默认为5s，修改为30s。
